{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G6saNLUFqHb"
   },
   "source": [
    "# Suggestion Detection using Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCWSEtNeGMsN"
   },
   "source": [
    "---\n",
    "\n",
    "This project involves tasks for feature engineering, training and evaluating a classifier for suggestion detection. We will work with the data from SemEval-2019 Task 9 subtask A to classify whether a piece of text contains a suggestion or not. \n",
    "\n",
    "\n",
    "The CSV file contains a header row followed by 5,440 rows in train.csv and 1,360 rows in test_seen.csv spread across 3 columns of data. Each row of data contains a unique id, a piece of text and a label assigned by an annotator. A label of $1$ indicates that the given text contains a suggestion while a label of $0$ indicates that the text does not contain a suggestion.\n",
    "\n",
    "You can find more details about the dataset in Sections 1, 2, 3 and 4 of [SemEval-2019 Task 9: Suggestion Mining from Online Reviews and Forums\n",
    "](https://aclanthology.org/S19-2151/).\n",
    "\n",
    "We will be using test_seen.csv for benchmarking our model, hence it has label. On the other hand, test_unseen is used for [Kaggle](https://www.kaggle.com/competitions/nlp2022ct5120suggestionmining/overview) competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ShQ2lPxmPfA4",
    "outputId": "f2e3a091-c45a-41f0-9dc0-ccf13b39b888"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  670k  100  670k    0     0  1612k      0 --:--:-- --:--:-- --:--:-- 1618k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  168k  100  168k    0     0   989k      0 --:--:-- --:--:-- --:--:-- 1001k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  208k  100  208k    0     0  1057k      0 --:--:-- --:--:-- --:--:-- 1072k\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/train.csv > train.csv\n",
    "!curl https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_seen.csv > test.csv\n",
    "!curl https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_unseen.csv > test_unseen.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5x0c38rCGk23"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file.\n",
    "train_df = pd.read_csv('train.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "test_df = pd.read_csv('test.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "# Store the data as a list of tuples where the first item is the text\n",
    "# and the second item is the label.\n",
    "train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n",
    "test_texts, test_labels = test_df[\"text\"].to_list(), test_df[\"label\"].to_list() \n",
    "\n",
    "# Check that training set and test set are of the right size.\n",
    "assert len(test_texts) == len(test_labels) == 1360\n",
    "assert len(train_texts) == len(train_labels) == 5440"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Scj45oSpdQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1: Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Pd8ed8NdlB_"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "> For pre-processing I have used the following methods:\n",
    "1. Tokenization: With this method we divide the data frame into small tokens. i.e we separate words from sentences for pre-processing.\n",
    "2. StopWordRemoval: Here we will be removing all the stop words in the list. Stopwords are the words that doesn't give much meaning to the sentence.\n",
    "3. Punctuation removal: We will be removing all the punctuations from the sentences.\n",
    "4. Text Lower Case: All the words in the list will be converted to lower.\n",
    "5. Detokenization: Finally we detokenize the list to convert the list of words into a single list of sentences.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jb7i3Le4aSYM",
    "outputId": "87069dbe-5b80-427e-8218-f953e19fb24c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5440\n",
      "1360\n",
      "1700\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts))\n",
    "print(len(test_texts))\n",
    "test_unseen = pd.read_csv(\"test_unseen.csv\", names=['id', 'text'], header=0)\n",
    "test_unseen_list = list(test_unseen['text'])\n",
    "print(len(test_unseen_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe68sOszD3qe"
   },
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "q9D48zhZrePD"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "train_tokens = []\n",
    "test_tokens = []\n",
    "testun_tokens = []\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "for i in train_texts:\n",
    "    train_tokens.append(tokenizer.tokenize(i))\n",
    "# print(train_tokens)\n",
    "\n",
    "for i in test_texts:\n",
    "    test_tokens.append(tokenizer.tokenize(i))\n",
    "# print(test_tokens)\n",
    "\n",
    "for i in test_unseen_list:\n",
    "    testun_tokens.append(tokenizer.tokenize(i))\n",
    "# print(testun_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_train = []\n",
    "for i in train_tokens:\n",
    "    out = []\n",
    "    for words in i:\n",
    "        out.append(stemmer.stem(words))\n",
    "    stemmed_train.append(out)\n",
    "# print(len(no_puncs_train))\n",
    "\n",
    "stemmed_test = []\n",
    "for i in test_tokens:\n",
    "    out = []\n",
    "    for words in i:\n",
    "        out.append(stemmer.stem(words))\n",
    "    stemmed_test.append(out)\n",
    "# print(len(no_puncs_test))\n",
    "\n",
    "stemmed_testun = []\n",
    "for i in testun_tokens:\n",
    "    out = []\n",
    "    for words in i:\n",
    "        out.append(stemmer.stem(words))\n",
    "    stemmed_testun.append(out)\n",
    "# print(len(no_puncs_testun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syhr3UxkEPhy"
   },
   "source": [
    "Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2vKWwj47rePE"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "no_stops_train = []\n",
    "no_stops_test = []\n",
    "\n",
    "for sentences in stemmed_train:\n",
    "    out = []\n",
    "    for word in sentences:\n",
    "        if word not in stops:\n",
    "            out.append(word)\n",
    "    no_stops_train.append(out)\n",
    "\n",
    "# print(no_stops_train)\n",
    "\n",
    "\n",
    "for sentences in stemmed_test:\n",
    "    out = []\n",
    "    for word in sentences:\n",
    "        if word not in stops:\n",
    "            out.append(word)\n",
    "    no_stops_test.append(out)\n",
    "\n",
    "# print(no_stops_test)\n",
    "\n",
    "no_stops_testun = []\n",
    "\n",
    "for sentences in stemmed_testun:\n",
    "    out = []\n",
    "    for word in sentences:\n",
    "        if word not in stops:\n",
    "            out.append(word)\n",
    "    no_stops_testun.append(out)\n",
    "# print(len(no_stops_testun))\n",
    "# print(no_stops_testun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_XlJb_XETEq"
   },
   "source": [
    "Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SZBN4gDprePF"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "no_puncs_train = []\n",
    "for i in no_stops_train:\n",
    "    out = []\n",
    "    for words in i:\n",
    "        out.append(re.sub(r'[^\\w\\s]', '', words))\n",
    "    no_puncs_train.append(out)\n",
    "# print(len(no_puncs_train))\n",
    "\n",
    "no_puncs_test = []\n",
    "for i in no_stops_test:\n",
    "    out = []\n",
    "    for words in i:\n",
    "        out.append(re.sub(r'[^\\w\\s]', '', words))\n",
    "    no_puncs_test.append(out)\n",
    "# print(len(no_puncs_test))\n",
    "\n",
    "no_puncs_testun = []\n",
    "for i in no_stops_testun:\n",
    "    out = []\n",
    "    for words in i:\n",
    "        out.append(re.sub(r'[^\\w\\s]', '', words))\n",
    "    no_puncs_testun.append(out)\n",
    "# print(len(no_puncs_testun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8bnc62LEZOZ"
   },
   "source": [
    "String Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnC4b5CarePF",
    "outputId": "7a398c28-1ee1-4956-afb2-5d5c5e22afb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5440\n",
      "1360\n",
      "1700\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "lower_train = []\n",
    "for i in no_puncs_train:\n",
    "    out = []\n",
    "    for words in i:\n",
    "        out.append(str.lower(words))\n",
    "    lower_train.append(out)\n",
    "print(len(lower_train))\n",
    "\n",
    "lower_test = []\n",
    "for i in no_puncs_test:\n",
    "    out = []\n",
    "    for words in i:\n",
    "        out.append(str.lower(words))\n",
    "    lower_test.append(out)\n",
    "print(len(lower_test))\n",
    "\n",
    "lower_testun = []\n",
    "for i in no_puncs_testun:\n",
    "    out = []\n",
    "    for words in i:\n",
    "        out.append(str.lower(words))\n",
    "    lower_testun.append(out)\n",
    "print(len(lower_testun))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvMUyz4bEgun"
   },
   "source": [
    "Detokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2r31V_YrePG",
    "outputId": "66d27064-5b58-40c4-d430-1107528c16de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5440\n",
      "1360\n",
      "1700\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordDetokenizer\n",
    "\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "train_detokens = []\n",
    "for i in stemmed_train:\n",
    "    train_detokens.append(detokenizer.detokenize(i))\n",
    "print(len(train_detokens))\n",
    "\n",
    "test_detokens = []\n",
    "for i in stemmed_test:\n",
    "    test_detokens.append(detokenizer.detokenize(i))\n",
    "print(len(test_detokens))\n",
    "\n",
    "testun_detokens = []\n",
    "for i in stemmed_testun:\n",
    "    testun_detokens.append(detokenizer.detokenize(i))\n",
    "print(len(testun_detokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IUJunnfXItQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 2: Feature Engineering (I) - TF-IDF as features\n",
    "\n",
    "Raw counts of words and `tf-idf` scores can be useful features for a classification task. We will use `tf-idf` scores as features for a Naïve Bayes classifier.\n",
    "\n",
    "After applying the preprocessing steps, we use the training data to train the classifier and make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gDsfB8xTGMg",
    "outputId": "eef5d9c1-71aa-4200-b41c-f7386b937c7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5272058823529412"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Calculate tf-idf scores for the words in the training set.\n",
    "\n",
    "cv = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "text_train_counts = cv.fit_transform(train_detokens)\n",
    "# print(text_train_counts)\n",
    "text_train_tfidf = tfidf.fit_transform(text_train_counts)\n",
    "\n",
    "# Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n",
    "\n",
    "naive_bayes = GaussianNB()\n",
    "naive_bayes.fit(text_train_counts.toarray(), train_labels)\n",
    "\n",
    "# Predict on the test set.\n",
    "predictions = []    # save your predictions on the test set into this list\n",
    "\n",
    "\n",
    "text_test_counts = cv.transform(test_detokens)\n",
    "text_test_tfidf = tfidf.transform(text_test_counts)\n",
    "\n",
    "predictions = naive_bayes.predict(text_test_tfidf.toarray())\n",
    "  \n",
    "\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "  '''\n",
    "  Calculate the accuracy score for a given set of predictions and labels.\n",
    "  \n",
    "  Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "    \n",
    "  '''\n",
    "\n",
    "  assert len(labels) == len(predictions)\n",
    "  \n",
    "  correct = 0\n",
    "  for label, prediction in zip(labels, predictions):\n",
    "    if label == prediction:\n",
    "      correct += 1 \n",
    "  \n",
    "  score = correct / len(labels)\n",
    "  return score\n",
    "\n",
    "# Calculate accuracy score for the classifier using tf-idf features.\n",
    "accuracy(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDx_M2aTIncl"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 3: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8jDzSU86xI1"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "> Accuracy cannot deal well with imbalanced data. For e.g if there are less number of positive results and more number of negative results then ML algorithm will support more number of negative results. I have used F1 score here because this method will provide better results as it works well with imbalanced data. F1 score will consider precision and recall both which will give better results than accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "UkUX5K0oMhKI",
    "outputId": "0b1835dc-2939-46e1-bfbf-8e4cf53256d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2950 \t Recall: 0.6697 \t Score: 0.4096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.4096'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(labels, predictions):\n",
    "\n",
    "    # check that labels and predictions are of same length\n",
    "    assert len(labels) == len(predictions)\n",
    "\n",
    "    score = 0.0\n",
    "\n",
    "    tp , fp, fn = 0,0,0\n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        if label == prediction:\n",
    "            if label == 1:\n",
    "                tp += 1\n",
    "        else:\n",
    "            if label == 1:\n",
    "                fn += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    print('Precision:', format(precision, '.4f'), '\\t', 'Recall:', format(recall, '.4f'), '\\t', 'Score:', format(score, '.4f'))\n",
    "\n",
    "\n",
    "    return format(score, '.4f')\n",
    "\n",
    "# Calculate evaluation score based on the metric of your choice for the classifier trained in Task 2 using tf-idf features.\n",
    "evaluate(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22OelF89a27J"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 4: Feature Engineering (II) - Other features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EBS0F877UyC"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "> To improve my accuracy we have specified ngram range and max_features in the count vectorizer. These features doesn't require any additional pre-processing steps. ngram range defines in how many terms we want to tokenize our data. For e.g. if ngram range is (1,1) then movie day word will be converted into 'movie', 'day'. But if the range is specified as (2,2) then the tokenized item will be 'movie day'. The max_features attribute will select the n number of terms with top frequencies. For e.g if max features is set to 5 then it'll select 5 most commonly used terms in the data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "u9mRku0va8kK",
    "outputId": "b69e1c3b-c2e7-4d82-ded8-32a1c9c0d971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7757352941176471\n",
      "Precision: 0.5598 \t Recall: 0.3934 \t Score: 0.4621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.4621'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create your features.\n",
    "cv = CountVectorizer(ngram_range=(2,2), max_features=65)\n",
    "tfidf = TfidfTransformer()\n",
    "text_train_counts = cv.fit_transform(train_detokens)\n",
    "text_train_tfidf = tfidf.fit_transform(text_train_counts)\n",
    "\n",
    "\n",
    "# Train a Naïve Bayes classifier using the features defined.\n",
    "naive_bayes = GaussianNB()\n",
    "naive_bayes.fit(text_train_counts.toarray(), train_labels)\n",
    "\n",
    "\n",
    "# Evaluate on the test set.\n",
    "predictions = []    # save predictions on the test set into this list\n",
    "\n",
    "\n",
    "text_test_counts = cv.transform(test_detokens)\n",
    "text_test_tfidf = tfidf.transform(text_test_counts)\n",
    "\n",
    "predictions = naive_bayes.predict(text_test_tfidf.toarray())\n",
    "\n",
    "print(accuracy(test_labels, predictions))\n",
    "evaluate(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyDD1zFQdwCf"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 5: Kaggle Competition\n",
    "\n",
    "Head over to https://www.kaggle.com/t/1f90b74da0b7484da9647638e22d1068  \n",
    "Use above classifier to predict the label for test_unseen.csv from competition page and upload the results to the leaderboard. The current baseline score is 0.36823. Make an improvement above the baseline. Please note that the evaluation metric for the competition is the f-score.\n",
    "\n",
    "Read competition page for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "JaC6B824Fe0H",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preparing submission for Kaggle\n",
    "kaggle_test_set = \"kaggle_test_set\"\n",
    "test_unseen = pd.read_csv(\"test_unseen.csv\", names=['id', 'text'], header=0)\n",
    "\n",
    "\n",
    "# model fitting\n",
    "cv = CountVectorizer(ngram_range=(2,2), max_features=65)\n",
    "tfidf = TfidfTransformer()\n",
    "text_testun_counts = cv.fit_transform(testun_detokens)\n",
    "text_testun_tfidf = tfidf.fit_transform(text_testun_counts)\n",
    "\n",
    "\n",
    "# predictions\n",
    "predictions = naive_bayes.predict(text_testun_tfidf.toarray())\n",
    "\n",
    "\n",
    "# Here Id is unique identifier assigned to each test sample ranging from test_0 till test_1699\n",
    "# Expected is a list of prediction made by your classifier\n",
    "sub = {\"Id\": [f\"test_{i}\" for i in range(len(predictions))],\n",
    "       \"Expected\": predictions}\n",
    "\n",
    "sub_df = pd.DataFrame(sub)\n",
    "\n",
    "# This will generate a test set which after uploading on kaggle competition will give a score for the competition\n",
    "sub_df.to_csv(f\"{kaggle_test_set}.csv\", sep=\",\", header=1, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6brptmkqY9C"
   },
   "source": [
    "Mention the approach that you have chosen briefly, and what is the mean average f-score that you have achieved? Did it improve above the chosen baseline model (0.36823)? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZQumdT-9yet"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "By performing various pre processing steps I was succesfully in achieving a 0.4090 f1 score in the step 3. In the step 4, I applied ngram and max_features attributes to the countvectorizer that increased the f1 score upto 0.5345. I specified the ngram range as (1,1) and max_features to consider as 18. With these values I am able to get the maximum of f1 score which is 0.5345 for my train and test data.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7eba71be4ce1d1c154237cb5824c1f921738735f10b9494334e971ae89f8c523"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
